{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3065b8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/class_march_24_gradient_descents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d38a21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31211030",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e3265",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d2fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb690c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_setosa = (y==0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678fef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal = X[:,(0,1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb1e43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af5e22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e19799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095edf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 5, tol = 1e-3, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6fdbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_sepal, y_setosa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = sgd_clf.intercept_/ sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2da21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38422411",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.coef_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d58f32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf.coef_.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039f6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c63c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858c0e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ w_{0} + w_{1}x_{1} + w_{2}x_{2} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814144d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ x_{2} = -\\frac{w_{1}}{w_{2}}x_{1} - \\frac{w_{0}}{w_{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9851de1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 100, tol = 1e-5, random_state = 42) \n",
    "sgd_clf.fit(X_sepal, y_setosa) \n",
    "import numpy as np \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )\n",
    "c,d = np.min(X[:,0]), np.max(X[:,0]) \n",
    "\n",
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  \n",
    "\n",
    "b = -sgd_clf.intercept_/ sgd_clf.coef_[0,1] \n",
    "\n",
    "ax.plot([c,d], [m*c + b, m*d+b]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04732bff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente para clasificación como función de un solo atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d212895",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  \n",
    "X,y = load_iris(return_X_y = True) \n",
    "y_setosa = (y==0) \n",
    "X_sepal = X[:,0] \n",
    "import matplotlib.pyplot as plt \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal,y_setosa.ravel() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365a41a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cost(w0):\n",
    "    y_pred = 1*(X_sepal<=w0)\n",
    "    return (1/len(X_sepal)*np.sum((y_setosa - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ce07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a,b = np.min(X_sepal), np.max(X_sepal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c70f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "W0 = np.linspace(a,b,400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab851f10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(W0, [cost(w) for w in W0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402e397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d40212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el valor de $w$ que minimiza la función de costo, hay una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d5ccf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **la ecuación normal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5531d46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "$$ \\hat{\\theta}  = (X^{T}X)^{-1} X^{T}y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963be76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:\n",
    "\n",
    "* $\\hat{w}$ es el valor de $w$ que minimiza la función de costo. \n",
    "\n",
    "* $y$ es el vector de valores objetivo que contiene $y^{(1)}$ a $y^{(m)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f736fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generemos algunos datos de aspecto lineal para probar esta ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e975abe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "X1 = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X1 + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78bc16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fe54d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.scatter(X1,y)\n",
    "plt.xlabel(r'$X_{1}$')\n",
    "plt.ylabel(r'$y$')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e2ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora calculemos $\\hat{w}$ usando la ecuación normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b910d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usaremos la función `inv()` del módulo de álgebra lineal de NumPy (`np.linalg`) para calcular la inversa de una matriz, y el método `dot()` para la multiplicación de matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853d86e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X1] # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d1e216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fed670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función que usamos para generar los datos es $y = 4 + 3x + \\text{ruido gaussiano}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842814d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos qué encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6d352a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd59f6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Habríamos esperado $w_{0} = 4$ y $w_{1} = 3$ en lugar de $w_{0} = 4.215$ y $w_{1} = 2.770$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dc32b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ded43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora podemos hacer predicciones usando $\\hat{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3167b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(w_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e2cdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tracemos las predicciones de este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610d47a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X1, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3fa11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Realizar una regresión lineal con Scikit-Learn es simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1797772",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X1, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96330d13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3515af9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase `LinearRegression` se basa en la función `scipy.linalg.lstsq()` (el nombre significa \"mínimos cuadrados\"), a la que puede llamar directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db051bfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12960f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta función calcula $\\hat{w}= X+y$, donde $X+$ es la pseudoinversa de $X$ (específicamente, la inversa de Moore-Penrose)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa79b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede usar `np.linalg.pinv()` para calcular el pseudoinverso directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd0972c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cf3a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La pseudoinversa en sí se calcula usando una técnica de factorización de matriz estándar llamada Descomposición de valor singular (SVD) que puede descomponer la matriz del conjunto de entrenamiento X en la multiplicación de tres matrices $U \\Sigma V$ (ver `numpy.linalg.svd()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127366e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El pseudoinverso se calcula como $X^{+} = V\\Sigma^{+}U^{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866cd9e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para calcular la matriz $\\Sigma^{+}$, el algoritmo toma $\\Sigma$ y establece en cero todos los valores menores que un pequeño valor de umbral, luego reemplaza todos los valores distintos de cero con su inversa y finalmente transpone la matriz resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1089f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este enfoque es más eficiente que calcular la Ecuación Normal, además maneja muy bien los casos límite: de hecho, la Ecuación Normal puede no funcionar si la matriz $X^{T}X$ no es invertible (es decir, singular), como si $ m < n$ o si algunas características son redundantes, pero la pseudoinversa siempre está definida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb772a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454f3bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación normal calcula el inverso de $X^{T}X$, que es una matriz $(n + 1) \\times (n + 1)$ (donde n es el número de características). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56614d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La complejidad computacional de invertir una matriz de este tipo suele ser de $O(n^{2.4})$ a $O(n^{3})$, según la implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, si duplica la cantidad de características, multiplica el tiempo de cálculo por aproximadamente $2^{2.4} = 5.3$ a $2^{3} = 8$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33117f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El enfoque SVD utilizado por la clase LinearRegression de Scikit-Learn es aproximadamente $O(n^{2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b464064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si duplica la cantidad de características, multiplica el tiempo de cálculo por aproximadamente 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c78bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ADVERTENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fba0ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tanto la ecuación normal como el enfoque SVD se vuelven muy lentos cuando la cantidad de características aumenta (por ejemplo, 100,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae146f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el lado positivo, ambos son lineales con respecto al número de instancias en el conjunto de entrenamiento (son $O(m)$), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que quepan en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9fce1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, una vez que haya entrenado su modelo de regresión lineal (usando la ecuación normal o cualquier otro algoritmo), las predicciones son muy rápidas: la complejidad computacional es lineal con respecto a la cantidad de instancias en las que desea hacer predicciones y la cantidad de características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85712478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, hacer predicciones sobre el doble de instancias (o el doble de funciones) llevará aproximadamente el doble de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9545fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora veremos una forma muy diferente de entrenar un modelo de regresión lineal, que se adapta mejor a los casos en los que hay una gran cantidad de funciones o demasiadas instancias de entrenamiento para caber en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57a401",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12f0f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Page: 173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d130acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Gradient Descent** es un algoritmo de optimización genérico capaz de encontrar soluciones óptimas a una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414944e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea general de **Gradient Descent** es ajustar los parámetros iterativamente para minimizar una **función de costo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38dc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suponga que está perdido en las montañas en una densa niebla, y solo puede sentir la pendiente del suelo debajo de sus pies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5d908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una buena estrategia para llegar rápidamente al fondo del valle es descender en dirección a la pendiente más pronunciada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es exactamente lo que hace Gradient Descent:  \n",
    "\n",
    "* mide el gradiente local de la función de error con respecto al vector de parámetros $\\theta$, y va en la dirección del gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d705dbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez que el gradiente es cero, ¡ha alcanzado un mínimo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dca0a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concretamente, empiezas llenando $\\theta$ con valores aleatorios (esto se llama inicialización aleatoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f8bf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego lo mejora gradualmente, dando un pequeño paso a la vez, cada paso intentando disminuir la función de costo (por ejemplo, el MSE), hasta que el algoritmo converge a un mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114c6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_3.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5ab10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un parámetro importante en Gradient Descent es el tamaño de los pasos, determinado por el hiperparámetro de tasa de aprendizaje (*learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190b96c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la tasa de aprendizaje es demasiado pequeña, el algoritmo tendrá que pasar por muchas iteraciones para converger, lo que llevará mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329c1c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_4_learnig_rate_small.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99122612",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por otro lado, si la tasa de aprendizaje es demasiado alta, es posible que saltes al otro lado del valle y termines en el otro lado, posiblemente incluso más alto de lo que estabas antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbf040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto podría hacer que el algoritmo diverja, con valores cada vez mayores, y no pueda encontrar una buena solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d689eb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_5_learning_rate_large.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3fbc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, no todas las **funciones de costos** se ven como tazones regulares y agradables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c20229",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede haber agujeros, crestas, mesetas y todo tipo de terrenos irregulares, lo que dificulta al mínimo la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e0696",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 4-6 muestra los dos desafíos principales con Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4cf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_6_global_minimum.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7e764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la inicialización aleatoria inicia el algoritmo a la izquierda, convergerá a un mínimo local, que no es tan bueno como el mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ce0b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si comienza a la derecha, tomará mucho tiempo cruzar la meseta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b79b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Y si te detienes demasiado pronto, nunca alcanzarás el mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fb0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, la función de costo de MSE para un modelo de regresión lineal resulta ser una función convexa, lo que significa que si selecciona dos puntos en la curva, el segmento de línea que los une nunca cruza la curva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490dc9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto implica que no hay mínimos locales, solo un **mínimo global**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227b8db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También es una función continua con una pendiente que nunca cambia abruptamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd40b9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos dos hechos tienen una gran consecuencia: se garantiza que Gradient Descent se acerque arbitrariamente al mínimo global (si espera lo suficiente y si la tasa de aprendizaje no es demasiado alta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d696973",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, la función de costo tiene forma de cuenco, pero puede ser un cuenco alargado si las características tienen escalas muy diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14640a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La figura 4-7 muestra el descenso de gradiente en un conjunto de entrenamiento donde las funciones 1 y 2 tienen la misma escala (a la izquierda) y en un conjunto de entrenamiento donde la función 1 tiene valores mucho más pequeños que la función 2 (a la derecha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c05c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_7.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca183046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como puedes ver, a la izquierda el algoritmo de Descenso de Gradiente va directo hacia el mínimo, alcanzándolo rápidamente, mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una marcha larga por un valle casi plano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edceb54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Eventualmente alcanzará el mínimo, pero tomará mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2badc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20482bda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al usar Gradient Descent, debe asegurarse de que todas las funciones tengan una escala similar (p. ej., usando la clase StandardScaler de Scikit-Learn), o de lo contrario tardará mucho más en converger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7698383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este diagrama también ilustra el hecho de que entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una **función de costo** (sobre el conjunto de entrenamiento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df70dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es una búsqueda en el espacio de parámetros del modelo: cuantos más parámetros tiene un modelo, más dimensiones tiene este espacio, y más difícil es la búsqueda: buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23b078",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja simplemente está en el fondo del recipiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58262923",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descenso de gradiente por lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb7f04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para implementar Gradient Descent, debe calcular el gradiente de la función de costo con respecto a cada parámetro del modelo $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19ff4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, debe calcular cuánto cambiará la función de costo si cambia $\\theta$ solo un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43ce7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A esto se le llama derivada parcial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537ab9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es como preguntar \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este?\" y luego haciendo la misma pregunta mirando hacia el norte (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af368d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación 4-5 calcula la derivada parcial de la función de costo con respecto al parámetro $\\theta$, $\\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta)$ anotado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\sum_{i = 1}^{m} \\left( \\theta^{T}x^{(i)} - y^{(i)}\\right)x_{j}^{(i)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c6bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de calcular estas derivadas parciales individualmente, puede usar la Ecuación 4-6 para calcularlas todas de una sola vez. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb5ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El vector gradiente, anotado $\\nabla_{\\theta} MSE(\\theta)$, contiene todas las derivadas parciales de la función de costo (una para cada parámetro del modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac84b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\nabla_{\\theta}MSE(\\theta) = \\begin{pmatrix}  \\frac{\\partial}{\\partial \\theta_{0}} MSE(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_{1}} MSE(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_{n}} MSE(\\theta) \\end{pmatrix} = \\frac{2}{m}X^{T}(X\\theta - y)   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b87569",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez que tenga el vector de gradiente, que apunta cuesta arriba, simplemente vaya en la dirección opuesta para ir cuesta abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd2934",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means subtracting $\\nabla MSE(\\theta)$ from $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e6256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aquí es donde entra en juego la tasa de aprendizaje $\\eta$: multiplique el vector gradiente por $\\eta$ para determinar el tamaño del paso cuesta abajo (Ecuación 4-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\theta^{\\text{next step}} = \\theta - \\eta \\nabla_{\\theta} MSE(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b192d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos una implementación rápida de este algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3622b773",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900b198",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡Eso no fue demasiado difícil! Veamos el theta resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe148d6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868f235",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡Oye, eso es exactamente lo que encontró la ecuación normal! Descenso de gradiente funcionó perfectamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b9b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero, ¿y si hubieras utilizado una tasa de aprendizaje eta diferente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e8c21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La siguiente figura muestra los primeros 10 pasos de Gradient Descent utilizando tres tasas de aprendizaje diferentes (la línea discontinua representa el punto de partida)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdaab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_8.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee80b89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la izquierda, la tasa de aprendizaje es demasiado baja: el algoritmo finalmente llegará a la solución, pero llevará mucho tiempo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96022df3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el medio, la tasa de aprendizaje parece bastante buena: en solo unas pocas iteraciones, ya ha convergido a la solución. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la derecha, la tasa de aprendizaje es demasiado alta: el algoritmo diverge, salta por todos lados y, de hecho, se aleja más y más de la solución en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc57a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To find a good learning rate, you can use grid search (see Chapter 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b64523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However,\n",
    "you may want to limit the number of iterations so that grid search can eliminate\n",
    "models that take too long to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1fbfff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may wonder how to set the number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eec47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If it is too low, you will still be far away from the optimal solution when the algorithm stops; but if it is too high, you will waste time while the model parameters do not change\n",
    "anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4009116",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny number $ϵ$ (called the tolerance)—because this happens when Gradient Descent has (almost) reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6866e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CONVERGENCE RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978bdff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $O(1/ϵ)$ iterations to reach the optimum within a range of $ϵ$, depending on the shape of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131ad49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you divide the tolerance by 10 to have a more precise solution, then the algorithm may have to run about 10 times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47141f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b33b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow\n",
    "when the training set is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63ddb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At the opposite extreme, Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85986c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc203f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (Stochastic GD can be implemented as an out-of-core algorithm; see Chapter 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61354003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902102b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down (see Figure 4-9). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21792d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So once the algorithm stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92f184",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_9.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2491e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando la función de costo es muy irregular (como en la Figura 4-6), esto puede ayudar al algoritmo a saltar fuera de los mínimos locales, por lo que Stochastic Gradient Descent tiene más posibilidades de encontrar el mínimo global que Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450ff86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403e32d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One solution to this dilemma is to gradually reduce the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4623d2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb91854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This process is akin to simulated annealing, an algorithm inspired from the process in\n",
    "metallurgy of annealing, where molten metal is slowly cooled down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d358b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function that determines the learning rate at each iteration is called the learning\n",
    "schedule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ccbbea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703a3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab279c19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este código implementa Stochastic Gradient Descent utilizando un programa de aprendizaje simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b7f052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0af06",
   "metadata": {},
   "source": [
    "## Referencias  \n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
