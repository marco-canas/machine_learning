{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3065b8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/machine_learning/blob/main/classes/class_march_17/clase_march_17_clasificacion_multietiqueta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d38a21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31211030",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e3265",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d2fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb690c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_setosa = (y==0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678fef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal = X[:,(0,1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb1e43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af5e22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e19799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095edf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 5, tol = 1e-3, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_sepal, y_setosa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = sgd_clf.intercept_/ sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38422411",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.coef_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d58f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.coef_.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039f6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c63c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858c0e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ w_{0} + w_{1}x_{1} + w_{2}x_{2} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814144d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ x_{2} = -\\frac{w_{1}}{w_{2}}x_{1} - \\frac{w_{0}}{w_{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9851de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 100, tol = 1e-5, random_state = 42) \n",
    "sgd_clf.fit(X_sepal, y_setosa) \n",
    "import numpy as np \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )\n",
    "c,d = np.min(X[:,0]), np.max(X[:,0]) \n",
    "\n",
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  \n",
    "\n",
    "b = -sgd_clf.intercept_/ sgd_clf.coef_[0,1] \n",
    "\n",
    "ax.plot([c,d], [m*c + b, m*d+b]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04732bff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente para clasificación como función de un solo atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d212895",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  \n",
    "X,y = load_iris(return_X_y = True) \n",
    "y_setosa = (y==0) \n",
    "X_sepal = X[:,0] \n",
    "import matplotlib.pyplot as plt \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal,y_setosa.ravel() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365a41a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cost(w0):\n",
    "    y_pred = 1*(X_sepal<=w0)\n",
    "    return (1/len(X_sepal)*np.sum((y_setosa - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = np.min(X_sepal), np.max(X_sepal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.linspace(a,b,400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab851f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(W0, [cost(w) for w in W0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaa439",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6402e397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d40212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el valor de $\\theta$ que minimiza la función de costo, hay una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d5ccf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **la ecuación normal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5531d46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-4. Normal Equation  \n",
    "\n",
    "$$ \\hat{\\theta}  = (X^{T}X)^{-1} X^{T}y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963be76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.  \n",
    "\n",
    "* $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f736fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generemos algunos datos de aspecto lineal para probar esta ecuación (Figura 4-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975abe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78bc16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e2ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let’s compute $\\hat{\\theta}$ using the Normal Equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b910d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the `inv()` function from NumPy’s linear algebra module (`np.linalg`) to compute the inverse of a matrix, and the `dot()` method for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fed670",
   "metadata": {},
   "source": [
    "The function that we used to generate the data is $y = 4 + 3x + \\text{Gaussian noise}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842814d",
   "metadata": {},
   "source": [
    "Let’s see what the equation found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ded43",
   "metadata": {},
   "source": [
    "We would have hoped for θ = 4 and θ = 3 instead of θ = 4.215 and θ = 2.770.\n",
    "Close enough, but the noise made it impossible to recover the exact parameters\n",
    "of the original function.\n",
    "Now we can make predictions using ˆ\n",
    "𝛉\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> X_new = np.array([[0], [2]])\n",
    ">>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    ">>> y_predict = X_new_b.dot(theta_best)\n",
    ">>> y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e2cdf",
   "metadata": {},
   "source": [
    "Let’s plot this model’s predictions (Figure 4-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3fa11",
   "metadata": {},
   "source": [
    "Performing Linear Regression using Scikit-Learn is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1797772",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> lin_reg = LinearRegression()\n",
    ">>> lin_reg.fit(X, y)\n",
    ">>> lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96330d13",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3515af9",
   "metadata": {},
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function\n",
    "(the name stands for “least squares”), which you could call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db051bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    ">>> theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa79b7",
   "metadata": {},
   "source": [
    "This function computes ˆ\n",
    "𝛉\n",
    "= X+y, where X+ is the pseudoinverse of X\n",
    "(specifically, the Moore-Penrose inverse). You can use np.linalg.pinv() to\n",
    "compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0972c0",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1089f3",
   "metadata": {},
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorization\n",
    "technique called Singular Value Decomposition (SVD) that can decompose the\n",
    "training set matrix X into the matrix multiplication of three matrices U Σ V (see\n",
    "numpy.linalg.svd()). The pseudoinverse is computed as X+ = VΣ+U⊺. To\n",
    "compute the matrix Σ+, the algorithm takes Σ and sets to zero all values smaller\n",
    "than a tiny threshold value, then it replaces all the nonzero values with their\n",
    "inverse, and finally it transposes the resulting matrix. This approach is more\n",
    "efficient than computing the Normal Equation, plus it handles edge cases nicely:\n",
    "indeed, the Normal Equation may not work if the matrix X X is not invertible\n",
    "(i.e., singular), such as if m < n or if some features are redundant, but the\n",
    "pseudoinverse is always defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb772a4",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120cda",
   "metadata": {},
   "source": [
    "The Normal Equation computes the inverse of X X, which is an (n + 1) × (n +\n",
    "1) matrix (where n is the number of features). The computational complexity of\n",
    "inverting such a matrix is typically about O(n ) to O(n ), depending on the\n",
    "implementation. In other words, if you double the number of features, you\n",
    "multiply the computation time by roughly 2 = 5.3 to 2 = 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b464064",
   "metadata": {},
   "source": [
    "The SVD approach used by Scikit-Learn’s LinearRegression class is about\n",
    "O(n ). If you double the number of features, you multiply the computation time\n",
    "by roughly 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c78bc",
   "metadata": {},
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85712478",
   "metadata": {},
   "source": [
    "Both the Normal Equation and the SVD approach get very slow when the number of features\n",
    "grows large (e.g., 100,000). On the positive side, both are linear with regard to the number of\n",
    "instances in the training set (they are O(m)), so they handle large training sets efficiently,\n",
    "provided they can fit in memory.\n",
    "Also, once you have trained your Linear Regression model (using the Normal\n",
    "Equation or any other algorithm), predictions are very fast: the computational\n",
    "complexity is linear with regard to both the number of instances you want to\n",
    "make predictions on and the number of features. In other words, making\n",
    "predictions on twice as many instances (or twice as many features) will take\n",
    "roughly twice as much time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9545fb",
   "metadata": {},
   "source": [
    "Now we will look at a very different way to train a Linear Regression model,\n",
    "which is better suited for cases where there are a large number of features or too\n",
    "many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57a401",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12f0f1",
   "metadata": {},
   "source": [
    "Page: 173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d130acc",
   "metadata": {},
   "source": [
    "Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414944e",
   "metadata": {},
   "source": [
    "The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38dc6a",
   "metadata": {},
   "source": [
    "Suppose you are lost in the mountains in a dense fog, and you can only feel the slope of the ground below your feet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5d908",
   "metadata": {},
   "source": [
    "A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6aa",
   "metadata": {},
   "source": [
    "This is exactly what Gradient Descent does: it measures the local gradient of the error\n",
    "function with regard to the parameter vector θ, and it goes in the direction of descending gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d705dbf",
   "metadata": {},
   "source": [
    "Once the gradient is zero, you have reached a minimum! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dca0a5",
   "metadata": {},
   "source": [
    "Concretely, you start by filling θ with random values (this is called random initialization). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f8bf4",
   "metadata": {},
   "source": [
    "Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum (see Figure 4-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114c6b1",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190b96c",
   "metadata": {},
   "source": [
    "An important parameter in Gradient Descent is the size of the steps, determined\n",
    "by the learning rate hyperparameter. If the learning rate is too small, then the\n",
    "algorithm will have to go through many iterations to converge, which will take a\n",
    "long time (see Figure 4-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329c1c5",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbf040",
   "metadata": {},
   "source": [
    "On the other hand, if the learning rate is too high, you might jump across the\n",
    "valley and end up on the other side, possibly even higher up than you were\n",
    "before. This might make the algorithm diverge, with larger and larger values,\n",
    "failing to find a good solution (see Figure 4-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d689eb0",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b79b7",
   "metadata": {},
   "source": [
    "Finally, not all cost functions look like nice, regular bowls. There may be holes,\n",
    "ridges, plateaus, and all sorts of irregular terrains, making convergence to the\n",
    "minimum difficult. Figure 4-6 shows the two main challenges with Gradient\n",
    "Descent. If the random initialization starts the algorithm on the left, then it will\n",
    "converge to a local minimum, which is not as good as the global minimum. If it\n",
    "starts on the right, then it will take a very long time to cross the plateau. And if\n",
    "you stop too early, you will never reach the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4cf4",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd40b9a",
   "metadata": {},
   "source": [
    "Fortunately, the MSE cost function for a Linear Regression model happens to be\n",
    "a convex function, which means that if you pick any two points on the curve, the\n",
    "line segment joining them never crosses the curve. This implies that there are no\n",
    "local minima, just one global minimum. It is also a continuous function with a\n",
    "slope that never changes abruptly. These two facts have a great consequence:\n",
    "Gradient Descent is guaranteed to approach arbitrarily close the global minimum\n",
    "(if you wait long enough and if the learning rate is not too high)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d696973",
   "metadata": {},
   "source": [
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl\n",
    "if the features have very different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14640a",
   "metadata": {},
   "source": [
    "Figure 4-7 shows Gradient Descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c05c9a",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca183046",
   "metadata": {},
   "source": [
    "As you can see, on the left the Gradient Descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edceb54",
   "metadata": {},
   "source": [
    "It will eventually reach the minimum, but it will take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2badc",
   "metadata": {},
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7698383",
   "metadata": {},
   "source": [
    "When using Gradient Descent, you should ensure that all features have a similar scale (e.g.,\n",
    "using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n",
    "This diagram also illustrates the fact that training a model means searching for a combination of model parameters that minimizes a cost function (over the training set). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df70dd",
   "metadata": {},
   "source": [
    "It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in 3\n",
    "dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23b078",
   "metadata": {},
   "source": [
    "Fortunately, since the cost function is convex in the case of Linear\n",
    "Regression, the needle is simply at the bottom of the bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0823",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost\n",
    "function with regard to each model parameter θ . In other words, you need to\n",
    "calculate how much the cost function will change if you change θ just a little bit.\n",
    "This is called a partial derivative. It is like asking “What is the slope of the mountain under my feet if I face east?” and then asking the same question facing\n",
    "north (and so on for all other dimensions, if you can imagine a universe with\n",
    "more than three dimensions). Equation 4-5 computes the partial derivative of the\n",
    "cost function with regard to parameter θ , noted \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9009f9b",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\sum   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c6bf0",
   "metadata": {},
   "source": [
    "Instead of computing these partial derivatives individually, you can use Equation\n",
    "4-6 to compute them all in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb5ab9",
   "metadata": {},
   "source": [
    "The gradient vector, noted $\\nabla_{\\theta} MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac84b3b",
   "metadata": {},
   "source": [
    "$$ \\nabla_{\\theta}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab9e5",
   "metadata": {},
   "source": [
    "Once you have the gradient vector, which points uphill, just go in the opposite\n",
    "direction to go downhill. This means subtracting ∇ MSE(θ) from θ. This is\n",
    "where the learning rate η comes into play: multiply the gradient vector by η to\n",
    "determine the size of the downhill step (Equation 4-7).\n",
    "\n",
    "$$ \\theta^{\\text{next step}} =  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b192d6",
   "metadata": {},
   "source": [
    "Let’s look at a quick implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900b198",
   "metadata": {},
   "source": [
    "That wasn’t too hard! Let’s look at the resulting theta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe148d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868f235",
   "metadata": {},
   "source": [
    "Hey, that’s exactly what the Normal Equation found! Gradient Descent worked perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e8c21",
   "metadata": {},
   "source": [
    "But what if you had used a different learning rate eta? Figure 4-8 shows the first 10 steps of Gradient Descent using three different learning rates (the dashed line represents the starting point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdaab6",
   "metadata": {},
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee80b89",
   "metadata": {},
   "source": [
    "On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96022df3",
   "metadata": {},
   "source": [
    "In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878377",
   "metadata": {},
   "source": [
    "On the right, the learning rate is too high: the algorithm diverges, jumping all over the\n",
    "place and actually getting further and further away from the solution at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc57a",
   "metadata": {},
   "source": [
    "To find a good learning rate, you can use grid search (see Chapter 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b64523",
   "metadata": {},
   "source": [
    "However,\n",
    "you may want to limit the number of iterations so that grid search can eliminate\n",
    "models that take too long to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4009116",
   "metadata": {},
   "source": [
    "You may wonder how to set the number of iterations. If it is too low, you will\n",
    "still be far away from the optimal solution when the algorithm stops; but if it is\n",
    "too high, you will waste time while the model parameters do not change\n",
    "anymore. A simple solution is to set a very large number of iterations but to\n",
    "interrupt the algorithm when the gradient vector becomes tiny—that is, when its\n",
    "norm becomes smaller than a tiny number ϵ (called the tolerance)—because this\n",
    "happens when Gradient Descent has (almost) reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6866e0",
   "metadata": {},
   "source": [
    "### CONVERGENCE RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978bdff",
   "metadata": {},
   "source": [
    "When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take O(1/ϵ) iterations to reach the optimum within a range of ϵ, depending on the shape of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131ad49",
   "metadata": {},
   "source": [
    "If you divide the tolerance by 10 to have a more precise solution, then the algorithm may have to run about 10 times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47141f54",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b33b49",
   "metadata": {},
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow\n",
    "when the training set is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21792d4",
   "metadata": {},
   "source": [
    "At the opposite extreme, Stochastic Gradient\n",
    "Descent picks a random instance in the training set at every step and computes\n",
    "the gradients based only on that single instance. Obviously, working on a single\n",
    "instance at a time makes the algorithm much faster because it has very little data\n",
    "to manipulate at every iteration. It also makes it possible to train on huge\n",
    "training sets, since only one instance needs to be in memory at each iteration\n",
    "(Stochastic GD can be implemented as an out-of-core algorithm; see Chapter 1).\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is\n",
    "much less regular than Batch Gradient Descent: instead of gently decreasing\n",
    "until it reaches the minimum, the cost function will bounce up and down,\n",
    "decreasing only on average. Over time it will end up very close to the minimum,\n",
    "but once it gets there it will continue to bounce around, never settling down (see\n",
    "Figure 4-9). So once the algorithm stops, the final parameter values are good,\n",
    "but not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e863bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d995c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7f052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7196c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ff744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f0af06",
   "metadata": {},
   "source": [
    "## Referencias  \n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
